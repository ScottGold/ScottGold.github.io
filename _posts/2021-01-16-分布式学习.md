# CAP理论

- Consistency 一致性。单调读一致性保证客户端不会读取到旧值，而单调写一致性则保证写操作是串行的。
- Availability 可用性
- Partition tolerance 分区容错性

有很多以写操作触发缓存更新的设计，它们通常又分为 **write back** 和 **write through** 两种模式。其中，**write back** 牺牲了更多的一致性，但带来了**更低的请求时延**。比如[第 4 课] 介绍过的 Linux 磁盘高速缓存就采用了 write back 这种设计，它虽然是单机内的一种缓存设计，但在分布式系统中缓存的设计方式也是一样的。而**write through** 会在**更新数据成功后再更新缓存**，虽然带来了很好的一致性，但写操作的时延会更久。

# AKF 立方体理论

专家们喜欢一套一套的东西，这理论真没什么特别，农民伯伯遇到这些情况也会这么做。

- X 轴：直接水平多开应用进程来扩展系统，加机器，多启动几个程序。
- Y 轴：将功能拆分出来扩展系统。
- Z 轴：基于用户信息扩展系统。分库分表。

# NWR算法

基于**鸽巢原理**，David K. Gifford 在 1979 年首次提出了Quorum 算法（参见《Weighted Voting for Replicated Data》论文），**解决去中心化系统冗余数据的一致性**问题。而 Quorum 算法提出，如果冗余数据存放在 N 个节点上，且每次写操作成功写入 W 个节点（其他 N - W 个节点将异步地同步数据），而读操作则从 R 个节点中选择并读出正确的数据，只要确保 W + R > N，同 1 条数据的读、写操作就不能并发执行，这样客户端就总能读到最新写入的数据。特别是当 W > N/2 时，同 1 条数据的修改必然是顺序执行的。这样，分布式系统就具备了强一致性，这也是 NWR 算法的由来。

当 N 为 5 时，如果系统读多写少时，可以将 W 设为 4，而 R 设为 2，这样读操作的性能会更好。

Dynamo 的开源版数据库Cassandra.

# 负载均衡

目前性能最好的 Nginx，以及在 Nginx 之上构建的 OpenResty，通常是第一选择。

通过 AKF 立方体 X 轴扩展系统时。三层（网络层）、四层（传输层）负载均衡都可用于扩展系统，甚至在单个局域网内你还可以使用二层（数据链路层）负载均衡。

基于 AKF Y 轴扩展系统时，负载均衡必须根据功能来分发请求。我们需要 Nginx 这样的七层（应用层）负载均衡。

# 一致性哈希

使用哈希算法扩展系统时，最大的问题在于代表哈希桶的服务器节点数发生变化时，哈希函数就改变了，数据与节点间的映射关系自然发生了变化，结果大量数据就得在服务器间迁移。特别是含有多份冗余数据的系统，迁移工作量更是会成倍提高。

我们必须解决分布式系统扩展时可能出现的两个问题：数据分布不均衡和访问量不均衡。

一致性哈希算法是通过以下 2 个步骤来建立数据与主机节点间映射关系的：

- 首先，将关键字经由通用的哈希函数映射为 32 位整型哈希值。这些哈希值会形成 1 个环，最大的数字 2^32 相当于 0。
- 其次，设集群节点数为 N，将哈希环由小至大分成 N 段，每个主机节点处理哈希值落在该段内的数据。**比如当节点数 N 等于 3 且均匀地分段时**， [0, 1/3∗2^32, 2/3∗2^32, 1*2^32]。也可以采用异构分段。

在真实的数据节点与哈希环之间引入一个虚拟节点层，解决扩容后导致各节点处理的数据不均衡的问题。比如 Nginx 的一致性哈希算法，每个权重为 1 的真实节点就含有160 个虚拟节点。

# 过期缓存：如何防止缓存被流量打穿？

为了提高命中率，缓存会基于时间、空间两个维度更新数据。在时间上可以采用 LRU(Least Recently Used)、FIFO 等算法淘汰数据，而在空间上则可以预读、合并连续的数据。

| 设备 | 耗时 |
| ---- | ---- |
|1个CPU周期　     | 0.3ns  |
|L1 cache　　     | 0.9ns  |
|L2 cache　　     | 2.8ns  |
|L3 cache　　     | 12.9ns |
|mem(cpu访问DRAM) | 120ns  |
|SSD　      　    | 50-150μs |
|机械磁盘寻道　　 | 1-10ms |
|机械磁盘读取1MB　| 60-150ms |
|互联网: 旧金山到纽约|　40ms |
|互联网：旧金山到英国|　81ms |
|互联网：旧金山到澳大利亚|183ms |
|TCP包重传　　    | 1-3s |
|OS虚拟机重启　　 | 4s   |
|SCSI命令超时　　 | 30s  |

通过多级缓存提升访问速度，前提命中率过得去。

Web 场景中，浏览器的本地缓存、操作系统内核中的 TCP 缓冲区、负载均衡中的 TLS 握手缓存、应用服务中的 HTTP 响应缓存、MySQL 中的查询缓存等，每一级缓存都缓解了上下游间不均衡的访问速度，通过缩短访问路径降低了请求时延，通过异步访问、批量处理提升了系统效率。当然，缓存使用了简单的 Key/Value 结构，因此可以用哈希表、查找树等容器做索引，这也提升了访问速度。

**当需要极致的利用各种硬件设备，需要去了解它们的构造。**

对于磁盘操作，还可以基于空间局部性原理，采用预读算法添加缓存数据（参考[第 4 讲] 介绍的 PageCache）。

写请求也可以更新缓存，你可以参考[第 20 讲] 我们介绍过 write through 和 write back 方式。其中，write back 采用异步调用回写数据，能通过批量处理提升性能。

LRU(Less Recently Used) 通常使用双向队列实现（时间复杂度为 O(1)），队首是最近访问的元素，队尾就是最少访问、即将淘汰的元素。当访问了队列中某个元素时，可以将其移动到队首。当缓存溢出需要淘汰元素时，直接删除队尾元素。

当热点缓存淘汰后，如果有大量相同的并发请求，可以在缓存结点记录所有的请求（合并请求），只需向服务提供应用程序发出一个请求，等响应后更新缓存，并回复所有请求。

## Nginx 是如何防止流量打穿缓存的？

Nginx 的**合并回源**功能开启后，Nginx 会将多个并发请求合并为 1 条回源请求，并锁住所有的客户端请求，直到回源请求返回后，才会更新缓存，同时向所有客户端返回响应。（合并回源，唉，这个词看得莫名其妙）

# 应用层多播：如何快速地分发内容？

gossip 协议（*gossip protocol*）又称 epidemic 协议（*epidemic protocol*），是基于流行病传播方式的节点或者进程之间信息交换的协议，在分布式系统中被广泛使用.

成千上万个节点的内容分发。可以采用接力传播， 如Gossip 这样的多播协议。和比特币那样。

网络层的 IP 多播功能有以下 4 个方面的问题：

- 从功能上看，IP 多播缺失了质量控制、可靠性传输等特性，无法满足绝大部分场景中的要求；
- 从管理上看，监控跨网络的多播报文并不容易，多播地址的管理也很复杂，而且多播很容易造成网络洪峰及安全问题；
- 从市场上看，单播报文在终端上的计费很成熟，相反，运营商对多播报文的计费要困难许多，相对缺乏推进动力；
- 从产业协作上看，IP 多播必须由各设备厂商的路由器、交换机配合，由于网络层是由内核实现的，所以还要同步升级操作系统。

阿里巴巴开源的[Dragonfly](https://github.com/DarLiner/Dragonfly) 蜻蜓是使用 HTTP 协议实现应用层的分发.Dragonfly中文名“蜻蜓”，是一个基于P2P的智能文件分发系统。解决了应用部署，大规模缓存文件分发，数据文件分发，图像分发等大规模文件分发场景中低效率，低成功率，浪费网络带宽等问题。

消息队列就具备了以下 7 个优点：

1. 降低了系统的耦合性。
2. 可伸缩性很容易实现。水平扩展容易。
3. 天然实现“削峰填谷”功能。
4. 提高了系统可用性。
5. 消息队列的生产者天然具备异步功能，这降低了生产者的请求处理时延，提升了用户体验。
6. 介绍过，基于 AKF Y 轴拆分功能可以降低数据规模，而且组件间分工更细也会带来更深入的性能优化。当消息队列作为通讯方式时，这种“事件驱动”的分布式系统很容易通过消息实现服务拆分，成本会低很多。
7. 消息队列服务对于各种消息的发布、消费情况都有统计，因此，从消息中就能获得业务的实时运行状态，以极低的成本实现系统的监控。

分布式文件系统

- GlusterFS（Cluster公司开发的POSIX分布式文件系统）
- GFS（Google，适合大文件存储）
- HDFS（参照GFS设计的）

流式计算.当面对持续实时产生动态数据的场景时，业务上通常需要在秒级时延中及时地拿到运算结果。

在数据库、HDFS 等分布式系统中存放的静态数据，由于拥有清晰的边界，所以被称为 InBound Data **有边界数据**。然而，线上运行中的互联网产品生命周期并不确定，它产生的数据有明确的开始，却没有截止时间点。对于这样有始无终的实时数据流，我们把它称为 OutBound Data **无边界数据**。

以时间驱动的固定窗口、滑动窗口和计数窗口，以及以事件驱动的会话窗口。为了避免乱序事件的影响，还可以通过携带超时时间的 Watermark 水位，基于事件发生时间更精准地划分窗口。

# [数据库选择](https://time.geekbang.org/column/article/267741?utm_term=pc_interstitial_815)

在分布式系统中，我们会同时使用多种数据库。比如，你可能会在 Redis 中存放用户 Session 会话，将业务数据拆解为由行、列构成的二维表存储在 MySQL 中，将需要全文检索的数据放在 ElasticSearch 中，将知识图谱放在 Neo4j 图数据库中，将数据量、访问量很大的数据放在 Cassandra 列式数据库或者 MongoDB 文档型数据库中，等等。

SQL数据库主要提供事务操作，符合数据库的范式。

NoSQL 数据库可以分为以下 4 类：

1. Key/Value 数据库，通常基于哈希表实现（参见[第 3 讲]），性能非常好。
2. 文档型数据库，在 Key/Value 数据库中，由于没有预定义的值结构，所以只能针对 Key 执行查询。
3. 列式数据库，比如[第 22 讲] 介绍过的 Cassandra。列式数据库基于 Key 来映射行，再通过列名进行二级映射，同时它基于列来安排存储的拓扑结构，这样当仅读写大量行中某个列时，操作的数据节点、磁盘非常集中，磁盘 IO、网络 IO 都会少很多。通过倒排索引实现了全文检索的 ElasticSearch，就适合使用列式存储存放 Doc Values，这样做排序、聚合时非常高效。
4. 图数据库，在社交关系、知识图谱等场景中，携带各种属性的边可以表示节点间的关系，由于节点的关系数量多，而且非常容易变化，所以关系数据库的实现成本很高，而图数据库既没有固定的数据模型，遍历关系的速度也非常快，很适合处理这类问题。







参考：

极客时间中陶辉的[系统性能调优必知必会](https://time.geekbang.org/column/intro/308?utm_term=pc_interstitial_815)，具体细节，建议去学习该课程。

